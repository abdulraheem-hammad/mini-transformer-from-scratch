{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvjdgHf9ekNBKu/dNsvOVb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulraheem-hammad/mini-transformer-from-scratch/blob/main/minitransfomer%2BBPE_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# ------------------------------------------\n",
        "# Utility functions for BPE\n",
        "# ------------------------------------------\n",
        "def get_stats(input_integers):\n",
        "    counts = {}\n",
        "    for pair in zip(input_integers, input_integers[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(input_integers, input_pair, new_token):\n",
        "    merged = []\n",
        "    i = 0\n",
        "    while i < len(input_integers):\n",
        "        if i < len(input_integers) - 1 and \\\n",
        "           input_integers[i] == input_pair[0] and \\\n",
        "           input_integers[i+1] == input_pair[1]:\n",
        "            merged.append(new_token)\n",
        "            i += 2\n",
        "        else:\n",
        "            merged.append(input_integers[i])\n",
        "            i += 1\n",
        "    return merged\n",
        "\n",
        "# ------------------------------------------\n",
        "# Updated BPETokenizer with save/load\n",
        "# ------------------------------------------\n",
        "class BPETokenizer:\n",
        "    def __init__(self):\n",
        "        self.merges = {}  # {(a,b): new_token_id}\n",
        "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
        "        self.base_vocab_size = 256\n",
        "        self.vocab_size = self.base_vocab_size\n",
        "\n",
        "    def train(self, text: str, target_vocab_size: int):\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        num_merges = target_vocab_size - self.base_vocab_size\n",
        "        for i in range(num_merges):\n",
        "            stats = get_stats(tokens)\n",
        "            if not stats:\n",
        "                break\n",
        "            pair = max(stats, key=stats.get)\n",
        "            new_token = self.base_vocab_size + i\n",
        "            tokens = merge(tokens, pair, new_token)\n",
        "            self.merges[pair] = new_token\n",
        "            self.vocab[new_token] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def encode(self, text: str):\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        for pair, idx in self.merges.items():\n",
        "            tokens = merge(tokens, pair, idx)\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        byte_seq = []\n",
        "        for t in tokens:\n",
        "            byte_seq.extend(self.vocab[t])\n",
        "        return bytes(byte_seq).decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "    def save(self, dirpath: str):\n",
        "        os.makedirs(dirpath, exist_ok=True)\n",
        "        merges_ser = {f\"{a}_{b}\": idx for (a, b), idx in self.merges.items()}\n",
        "        with open(os.path.join(dirpath, \"merges.json\"), \"w\") as f:\n",
        "            json.dump(merges_ser, f)\n",
        "        meta = {\"vocab_size\": self.vocab_size, \"base_vocab_size\": self.base_vocab_size}\n",
        "        with open(os.path.join(dirpath, \"meta.json\"), \"w\") as f:\n",
        "            json.dump(meta, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, dirpath: str):\n",
        "        tok = cls()\n",
        "        with open(os.path.join(dirpath, \"merges.json\"), \"r\") as f:\n",
        "            merges_ser = json.load(f)\n",
        "        tok.merges = {tuple(map(int, k.split(\"_\"))): v for k, v in merges_ser.items()}\n",
        "        for (a, b), idx in tok.merges.items():\n",
        "            tok.vocab[idx] = tok.vocab[a] + tok.vocab[b]\n",
        "        with open(os.path.join(dirpath, \"meta.json\"), \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "        tok.vocab_size = meta[\"vocab_size\"]\n",
        "        tok.base_vocab_size = meta[\"base_vocab_size\"]\n",
        "        return tok\n",
        "\n",
        "# ------------------------------------------\n",
        "# Configuration and sampling\n",
        "# ------------------------------------------\n",
        "INPUT_FILE    = \"input.txt\"\n",
        "TOKENIZER_DIR = \"./tokenizer\"\n",
        "SAMPLE_SIZE   = 20 * 1024 * 1024   # 20 MB sample\n",
        "MAX_TOKENS    = 5_000_000         # use first 5M tokens for training\n",
        "TARGET_VOCAB  = 2000              # desired vocab size\n",
        "\n",
        "# Read sample of the corpus\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    text = f.read(SAMPLE_SIZE)\n",
        "\n",
        "# Initialize or load tokenizer\n",
        "if os.path.isdir(TOKENIZER_DIR) and os.path.isfile(os.path.join(TOKENIZER_DIR, \"merges.json\")):\n",
        "    print(\"Loading existing tokenizer...\")\n",
        "    tokenizer = BPETokenizer.load(TOKENIZER_DIR)\n",
        "else:\n",
        "    print(\"Training new tokenizer on sample...\")\n",
        "    tokenizer = BPETokenizer()\n",
        "    tokenizer.train(text, TARGET_VOCAB)\n",
        "    tokenizer.save(TOKENIZER_DIR)\n",
        "\n",
        "# Encode sampled text and limit tokens\n",
        "all_tokens = tokenizer.encode(text)\n",
        "if len(all_tokens) > MAX_TOKENS:\n",
        "    all_tokens = all_tokens[:MAX_TOKENS]\n",
        "\n",
        "data = torch.tensor(all_tokens, dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "\n",
        "# ------------------------------------------\n",
        "# Transformer hyperparameters adjusted\n",
        "# ------------------------------------------\n",
        "vocab_size    = tokenizer.vocab_size\n",
        "batch_size    = 64\n",
        "block_size    = 128\n",
        "max_iters     = 500\n",
        "eval_interval = 50\n",
        "learning_rate = 3e-4\n",
        "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters    = 20\n",
        "n_embd        = 128\n",
        "n_head        = 4\n",
        "n_layer       = 4\n",
        "dropout       = 0.0\n",
        "\n",
        "# ------------------------------------------\n",
        "# Data loading functions\n",
        "# ------------------------------------------\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size]     for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# ------------------------------------------\n",
        "# Model definition\n",
        "# ------------------------------------------\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x); q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1); wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd), nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa    = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd  = FeedForward(n_embd)\n",
        "        self.ln1   = nn.LayerNorm(n_embd)\n",
        "        self.ln2   = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f   = nn.LayerNorm(n_embd)\n",
        "        self.lm_head= nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "        B,T,C = logits.shape\n",
        "        loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# ------------------------------------------\n",
        "# Training loop\n",
        "# ------------------------------------------\n",
        "model     = BigramLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "print(f\"Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "for it in range(max_iters):\n",
        "    if it % eval_interval == 0 or it == max_iters-1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Generation\n",
        "# ------------------------------------------\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "output  = model.generate(context, max_new_tokens=2000)[0].tolist()\n",
        "print(decode(output))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkPAdr57FnLB",
        "outputId": "ede2a62f-5f60-420d-84b3-07d3d040c63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing tokenizer...\n",
            "Model params: 1.32M\n",
            "step 0: train loss 7.7654, val loss 7.7698\n",
            "step 50: train loss 6.9903, val loss 7.0445\n",
            "step 100: train loss 6.9448, val loss 7.0152\n",
            "step 150: train loss 6.9127, val loss 6.9796\n",
            "step 200: train loss 6.7710, val loss 6.8681\n",
            "step 250: train loss 6.4937, val loss 6.6300\n",
            "step 300: train loss 6.1057, val loss 6.2508\n",
            "step 350: train loss 5.7784, val loss 5.9345\n",
            "step 400: train loss 5.5009, val loss 5.6673\n",
            "step 450: train loss 5.2684, val loss 5.4609\n",
            "step 499: train loss 5.0939, val loss 5.2893\n",
            "\u0000branes. He reyellow cheered. \"ok how to piced catssailed his y and did be ever ed to like the Max. The kitchens that plealato the pock anyso wave. They would take the e. But he nd and the love they got lot of hug looked very happy.\n",
            "When he could peired wrong, wtakenmake happillegluwilk with a no that. We was so proud of hneeded to the fish. I will find ed it? Twes a rainy. He drelffunnearing her of some proud of herer in Me, hey .\n",
            "\"Ben, not .\n",
            "\"Look, Timmy kept branbeaed them with a cat and find the baard it was too watched the posliwas always his mom said bed.\"  away each cirscard that Tom tried to little flower his hand. She to. The raes was playing inly ba his mom back to the day and like for feeling a was glad you. They listened to the Samy again. She ilday, she had went to want to put on girl. It started he asked for better pa. He loved to moplained and Ben fell play er. He sky was dog. He wanted to you I named teato Sara: puppy good what happy from ground: shouted that Lurpilcref always momshouted to be ft on the floor. Max. Bit. The carle on the ved a sees bahat's friend to milet's would sm. She thinkcy inside. She said, \"Ben and Lucy if she floor. You are for the special smileds back to the surpris.\n",
            "Lilencoulot of back to his climb as beautiful, he liked they quitable. You castreeverykeed to rema rock am from it waspon.\n",
            "\"Jan. He said, \"decides and good mamy, him the te. The tablemen, said le. You hurtm back. He put on the car's mommy puand other . It deciding, \"I can where the beoctt. He knew she night, dad endfter, and her mom was walked.\"\n",
            "ought the botting the lealy picworld. Every mor, sad and see a harowne and was so happy too.\"\n",
            "'t low. They wanted to goe, Lily and do Sue downer too a lot of it and every gry. He saw a for her cleaned him it shape of the store alury, box them, but she was put it fincar.\n",
            "After to his marying at what they all rememberbetter and Ben are sadurer and ed to sorry. Jells they all looked at happened, a proud toounging with their say, \", \"toucher not you can friends wasend like happy sto. Sam is better gives fice, the rockept Anna are to help aost. She said she swinged it's truck.\n",
            "<|endoftext|>\n",
            "One day, Tom was so excited to swimbun. \"It's okay, hands and happy. But he was so happyyar me animal. He did piring her to play with the. Twintdoll.\n",
            "<|endoftext|>\n",
            "One day, she felt she was that, Lily and Tom was surpriddeticed the phlick with his way is arnewto a writted with a big monkey sener a small falla near a day You're differly and share she tria did not ferg snow.\n",
            "The aNo, you it's wab for hidle. They are happnd and said she liked to show, stayof. Sara it to erly he grabb, the cut himy, mum. \"Lila hagain. TLuvery much?\" remembers to untle to drifeat. Now differbb. As it in the to her swecamis a with a sm, Tom, Tim lovsh hard. They laughed and is a traspet into the special wanted to see tast a ball barks were veryand that he was an having to her toys with boy named Timmy. Timmy took a swing arint on the big, right lot.\n",
            "When go to find but swe, \"this gomix to feel s. She look a \u000bLookerabout he came careful;s. He will peeit and even kept two friends with good going Timmy's notic her t. They sey apeilly!\n",
            "Let's laugh a ball her can have a wiTimmy's way the best kitchen, try.\n",
            "<|endoftext|>\n",
            "One day, Be.\n",
            "They cheeres with. He couldn't find withon the ad. But I.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little girl named Lily. She broughet. They back whe and on itl down that she was wory ash, home ait. He was ed him to make  a toys you grabbed the taly en ask gues into the go to the mom about his for its water er, they ticks they got pp. He got her ahappened.\n",
            "Lily asked, \"promislike the to it. Her momturned if he then he like it!other it!\"\n",
            "pleaback to the brave o it. He went swimcame out.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a caiss right. Thelsorry! omen, d and ped on the being that the wasingsed him not windowfavormom so both m away.\n",
            "\"Wittes, and just play that the rain.\n",
            "Gchonso, ony ches his animal. They were playing with his friend try it. But every day again. That reached some showed her not dad there even thankhave a toys and seens and the hody it un him with play with the way to y. Sleat to happy she �sain her go se take his cars. They his favorite they timal walked of the lay of the pally, cake just he had to places, and he had to wner. They all is a ey helped Janliminel help him. She looked at offW�y boaty and careful can Mom, kided them. The bird it.\n",
            "Suddenly, they sayed, it keep it stomy and shin, Lily L him at the most magical teaound he would pain.\n",
            "Suddenly, Lily was a coupuppluin's feel safe and a is ames and she came water. Awent to tree and started to leacrasfor a e, you can sts and says.\n",
            "They got can explore of the riy, but we can scared. Tspecial things happke.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a gfeed it with a truck and take his mom, her at the ted and things.\n",
            "<|endoftext|>\n",
            "One day, she couldn't window'laed her being might decid,\" Ben land himtable, luincisbeautiful from happy. He is the m and a how foo.\n",
            "The red was very happy to hear tweroabranMiinsidpet the end bluine. She can them. You said. \"You three year with!\n",
            "The family was a sad. The mu!\"\n",
            "The wasrom op. The �. Max full see a big picbook e, Timd of the flas, fr. Lily, at rand over. Thank foron played with about mom and . They aice not eime. They have to piecen to around anyany mom to helpDo you want to ed him fows ted. They said \"city in very much.\n",
            "<|endoftext|>\n",
            "One day, Maybe the mout.\n",
            "After slide and happy to have make forest. He took the best I can make \"No, fed with the mared that cuts she Ben laughry buAnny. She had to. When he even very happy fun about some wred to make that he ke a tent, but felt very little girl named play we can tookluon fohead. She was anTom. Sener.\n",
            "<|endoftext|>\n",
            "Once upon a time there was a animals and stalight.\n",
            "<|endoftext|>\n",
            "Once upon a time there was a bip to the safe of the little boy back to the friend.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little girl named Lily. She my and home. I a s, the bunmy and one lsed, \"you. From then on, Tomisry his wating when I blem up mes! You are quick adventure. Inuntil it was over to Lily's cat is stronget and picked y. They looked at the important to frop and more saw ded lots of pretthaty!\"\n",
            "Sum had lots of friends. They family boats.\n",
            "<|endoftext|>\n",
            "One day, he jumparid for it noises with the stama loved to forwoonose. They had new they realized a bo. Tom more had boy heard aOKs and scared. From that day on, she was happy naughmy was want to pept the lion. The man is a little girl named we runitty.\n",
            "\"Bens were vuming at behind ld. And help tewinstatoy was so happy to get some wormMr-y, stoy.\"\n",
            "Lily likway to be juwatchrom ss to go home are tey, she wants to fly down. It prostore it end toyses he had to e, Sara gave it. She started to z. It is ading. Ined him t and Liled Sam, the special bolmy are Ros to up me bird le when e was camellel.\n",
            "They know Lily quickly that liked to play in the day fe.\n",
            "TTim and Lily couldbunny was a fastet and Mr, to use for fes and making thing. He told ist la anye! He tims. Let's have a it's No, to Sam and play and the truck. Everyone was very bofuls and drer, can told him to keep hars to play with his me.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little girl named Lily. She dre. They were sad because she looking the poown new Spot wha. Browntrawith his ightwehis toyes, the mom that she learned that mom was love tree withgood you can dad her yellow man were the necks.\n",
            "\"Don't worer.\n",
            "The trake and like to use youes with the beamiwhite .\n",
            "<|endoftext|>\n",
            "One day, Spotarpulled up to feel better, Tim againgirl. W. They set the end ity pafock! That's wrong to give in beelep hom he couldn't find bath. The twwe \u0015inside around your . It is not never dinwas very proud of spon, when il, But sorry, pictI told her manny were how friend Max listen. They su. We gs together, tried to I runful, �cloging the beaorion and not friendlyed Billy Rt?\" The blocks and magicthe best she pictse we could use round all the planchese. Mt to by, a cry h in the be st. It says he grabbed. He liked to sway to find too. She could play with itt to walked untingwelstant Sam. He broughving elcaful. He lifted a food sister.\n",
            "Jorn arite backsur. She did not know she saw a nice shars.\n",
            "<|endoftext|>\n",
            "One day, they hopy and I after you.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little girl named Lily. She �said, story for hear little fused. It topmy. You saye!\" play with seink to not ouse to turns the c. They wished thinget to the gre! e, there was a evy abreakisrock in his thingi. They go to the mom fromforest. The win and chaimportant to be your cu. But ello, they try.\n",
            "prettback to the car. His follows. He rey lif again, but she er and cloed with mist if they puty going to the m!\n",
            "The elsgging and he lov amazle itwa hurt. It decided more around k. From then on, Why in the garden.\n",
            "Timmy Sam wanted to clime was time out of the liked to decided to has little friend long and happy something ellack, so she is awa araced into ke, but they aforch into the pilly je!\" Ahe \n"
          ]
        }
      ]
    }
  ]
}